{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 09.Classification pt.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name:\n",
    "\n",
    "Date: 17 November 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Naive Bayes (discrete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement Multi-variate Bernoulli NB for smap filtering.\n",
    "\n",
    "- Category $0$: Ham\n",
    "- Category $1$: Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.a - preprocess\n",
    "\n",
    "Use the collection of documents stored in train_ham, train_spam directories to select an appropriate set of features-words, based on frequency of appearence in the text. Select a maximum of $N=40$ tokens per category. You can use CountVectorizer from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_tokens_per_class = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Fishbone\n"
     ]
    }
   ],
   "source": [
    "# Your current path\n",
    "%cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '#YOUR PATH HERE'\n",
      "/Users/Fishbone\n",
      "mkdir: data: File exists\n",
      "/Users/Fishbone/data\n",
      "\u001b[34m09_exercise_Spam\u001b[m\u001b[m/     09_exercise_Spam.zip\n",
      "--2021-11-17 19:21:56--  https://nuage.lip6.fr/s/bDM42aFWe5BxYRw/download/09_exercise_Spam.zip\n",
      "Resolving nuage.lip6.fr (nuage.lip6.fr)... 132.227.201.11\n",
      "Connecting to nuage.lip6.fr (nuage.lip6.fr)|132.227.201.11|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7342887 (7.0M) [application/zip]\n",
      "Saving to: ‘09_exercise_Spam.zip.1’\n",
      "\n",
      "09_exercise_Spam.zi 100%[===================>]   7.00M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2021-11-17 19:21:56 (52.3 MB/s) - ‘09_exercise_Spam.zip.1’ saved [7342887/7342887]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# change directory to where you will store the dataset\n",
    "%cd #YOUR PATH HERE\n",
    "%mkdir data\n",
    "%cd data\n",
    "%ls\n",
    "!wget https://nuage.lip6.fr/s/bDM42aFWe5BxYRw/download/09_exercise_Spam.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzip\n",
    "!tar -zxvf 09_exercise_Spam.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m09_exercise_Spam\u001b[m\u001b[m/     09_exercise_Spam.zip\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Fishbone/Documents/data/09_exercise_Spam\n"
     ]
    }
   ],
   "source": [
    "%cd 09_exercise_Spam/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Module\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text File\n",
    "\n",
    "def read_text_file(file_path,out):\n",
    "    with open(file_path,mode='r',encoding=\"utf8\", errors='ignore') as f:\n",
    "        data = f.read()\n",
    "        out.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First the train_ham**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_ham = '/train_ham'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the directory\n",
    "path = os.getcwd()+path_train_ham\n",
    "os.chdir(path)\n",
    "  \n",
    "# iterate through all TRAIN files\n",
    "TEXT_train_ham = []\n",
    "for file in os.listdir():\n",
    "    # Check whether file is in text format or not\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = f\"{path}/{file}\"\n",
    "  \n",
    "        # call read text file function\n",
    "        read_text_file(file_path,TEXT_train_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: summer cookout\n",
      "sally ,\n",
      "i had talked to ted and he mentioned the cookout you were planning for the ou\n",
      "group this summer . please let me help with any of the logistics ,\n",
      "set - up , and clean up . i know these things require a great deal of effort ,\n",
      "especially when you welcome people to your personal residence . i want to ,\n",
      "and insist on helping in whatever way i can . i am really grateful for what\n",
      "you have done for the university of oklahoma and i am also aware of\n",
      "the great sacrifices you make for enron . if possible , we might schedule a\n",
      "lunch to discuss the specifics . i would also be grateful if we could discuss\n",
      "my most recent opportunity at enron , texas risk . i realize the there is a\n",
      "great deal i can gain from this move and i would appreciate any insight on how\n",
      "to obtain the most from this experience .\n",
      "thanks\n",
      "jody\n"
     ]
    }
   ],
   "source": [
    "print(TEXT_train_ham[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=CountVectorizer(max_features=N_tokens_per_class,stop_words='english',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '10', '11', '12', '2000', 'attached', 'beck', 'business', 'cc', 'corp', 'date', 'ect', 'enron', 'following', 'forwarded', 'group', 'hou', 'houston', 'information', 'know', 'let', 'like', 'management', 'meeting', 'need', 'new', 'office', 'pm', 'process', 'questions', 'report', 'review', 'risk', 'sally', 'subject', 'team', 'thanks', 'time', 'week', 'work']\n"
     ]
    }
   ],
   "source": [
    "X_ham=vectorizer.fit(TEXT_train_ham)\n",
    "TOKENS_ham = vectorizer.get_feature_names()\n",
    "print(TOKENS_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Then the train_spam**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Fishbone/Documents/data/09_exercise_Spam\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "path_train_spam = '/train_spam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the directory\n",
    "path = os.getcwd()+path_train_spam\n",
    "os.chdir(path)\n",
    "  \n",
    "# iterate through all files\n",
    "TEXT_train_spam = []\n",
    "for file in os.listdir():\n",
    "    # Check whether file is in text format or not\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = f\"{path}/{file}\"\n",
    "  \n",
    "        # call read text file function\n",
    "        read_text_file(file_path,TEXT_train_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: in the heart of your business !\n",
      "corporate image can say a lot of things about your\n",
      "company . contemporary rhythm of life is too dynamic . sometimes it takes only\n",
      "several seconds for your company to be remembered or to be lost amonq\n",
      "competitors . get your ioqo , business stationery or website done riqht\n",
      "now ! fast turnaround : you wiil see severai iogo variants in three\n",
      "business days . satisfaction guaranteed : we provide uniimited amount of\n",
      "changes ; you can be sure : it wili meet your needsand fit your\n",
      "business . fiexible discounts : iogo improvement , additional formats , buik\n",
      "orders , special packages . creative design for competitive price : have a look at it right\n",
      "now ! _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ not interested . . . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n"
     ]
    }
   ],
   "source": [
    "print(TEXT_train_spam[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '10', 'address', 'best', 'business', 'click', 'com', 'company', 'don', 'email', 'free', 'future', 'http', 'information', 'just', 'know', 'life', 'like', 'list', 'mail', 'make', 'message', 'money', 'need', 'net', 'new', 'offer', 'online', 'order', 'receive', 'save', 'site', 'software', 'subject', 'time', 'use', 'want', 'way', 'website', 'www']\n"
     ]
    }
   ],
   "source": [
    "X_spam=vectorizer.fit(TEXT_train_spam)\n",
    "TOKENS_spam = vectorizer.get_feature_names()\n",
    "print(TOKENS_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.b - done\n",
    "\n",
    "Then use the intersection of tokens from both categories to build a bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS = set(TOKENS_ham).union(set(TOKENS_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hou', '10', 'meeting', 'order', 'address', 'need', 'net', 'way', 'www', 'http', 'online', 'corp', 'business', 'company', 'list', 'group', 'future', 'like', 'let', 'sally', 'software', 'receive', 'attached', 'money', '00', 'free', '11', 'email', 'message', 'know', 'pm', 'houston', 'office', 'com', 'questions', 'report', 'team', 'offer', 'mail', 'ect', 'time', 'process', 'following', 'work', 'subject', 'best', '2000', 'click', 'thanks', 'new', 'week', 'want', '12', 'management', 'site', 'use', 'date', 'beck', 'website', 'just', 'save', 'review', 'cc', 'risk', 'information', 'life', 'forwarded', 'don', '000', 'enron', 'make'} \n",
      "\n",
      "count_TOKENS= 71\n"
     ]
    }
   ],
   "source": [
    "print(TOKENS,'\\n')\n",
    "print('count_TOKENS=',len(TOKENS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common_TOKENS= {'10', 'time', 'need', 'subject', 'business', 'know', 'information', 'like', 'new'}\n"
     ]
    }
   ],
   "source": [
    "print('common_TOKENS=',set(TOKENS_ham).intersection(set(TOKENS_spam)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the set of TOKENS, to create a binary matrix for the Train and a binary matrix for the Test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "My_vectorizer = CountVectorizer(vocabulary=TOKENS,binary='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ham_my = My_vectorizer.fit_transform(TEXT_train_ham).toarray()\n",
    "X_spam_my = My_vectorizer.fit_transform(TEXT_train_spam).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 71)\n",
      "(3565, 71)\n"
     ]
    }
   ],
   "source": [
    "#Test if the size is correct\n",
    "print(np.shape(X_ham_my))\n",
    "print(np.shape(X_spam_my))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spam [Y=1]</th>\n",
       "      <th>hou</th>\n",
       "      <th>10</th>\n",
       "      <th>meeting</th>\n",
       "      <th>order</th>\n",
       "      <th>address</th>\n",
       "      <th>need</th>\n",
       "      <th>net</th>\n",
       "      <th>way</th>\n",
       "      <th>www</th>\n",
       "      <th>...</th>\n",
       "      <th>review</th>\n",
       "      <th>cc</th>\n",
       "      <th>risk</th>\n",
       "      <th>information</th>\n",
       "      <th>life</th>\n",
       "      <th>forwarded</th>\n",
       "      <th>don</th>\n",
       "      <th>000</th>\n",
       "      <th>enron</th>\n",
       "      <th>make</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4960</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4961</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4962</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4963</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4964</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4965 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Spam [Y=1]  hou  10  meeting  order  address  need  net  way  www  ...  \\\n",
       "0              0    0   0        1      0        0     1    0    0    0  ...   \n",
       "1              0    0   0        0      0        0     0    0    0    0  ...   \n",
       "2              0    0   0        0      0        0     0    0    0    0  ...   \n",
       "3              0    0   0        0      0        1     1    0    0    1  ...   \n",
       "4              0    0   0        1      0        0     1    0    0    1  ...   \n",
       "...          ...  ...  ..      ...    ...      ...   ...  ...  ...  ...  ...   \n",
       "4960           1    0   0        0      0        0     0    1    0    0  ...   \n",
       "4961           1    0   0        0      1        0     0    0    0    0  ...   \n",
       "4962           1    1   1        1      1        0     0    1    1    0  ...   \n",
       "4963           1    0   0        0      0        0     0    0    0    0  ...   \n",
       "4964           1    0   1        0      0        0     0    0    0    0  ...   \n",
       "\n",
       "      review  cc  risk  information  life  forwarded  don  000  enron  make  \n",
       "0          0   0     1            0     0          0    0    0      0     0  \n",
       "1          0   1     0            0     1          1    0    0      0     0  \n",
       "2          1   1     1            0     0          0    0    1      1     0  \n",
       "3          1   1     0            0     0          0    0    0      1     0  \n",
       "4          1   1     0            0     0          0    0    0      1     0  \n",
       "...      ...  ..   ...          ...   ...        ...  ...  ...    ...   ...  \n",
       "4960       1   0     1            1     0          0    1    0      0     1  \n",
       "4961       0   0     0            0     0          0    0    0      0     0  \n",
       "4962       0   0     0            0     0          0    0    1      0     0  \n",
       "4963       0   0     0            0     0          0    0    0      0     0  \n",
       "4964       0   0     0            0     0          1    0    0      0     0  \n",
       "\n",
       "[4965 rows x 72 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_DATA_ham = pd.DataFrame(X_ham_my,columns=TOKENS)\n",
    "df_DATA_ham.insert(0, 'Spam [Y=1]',0)\n",
    "df_DATA_spam = pd.DataFrame(X_spam_my,columns=TOKENS) \n",
    "df_DATA_spam.insert(0, 'Spam [Y=1]',1)\n",
    "df_DATA = pd.concat([df_DATA_ham,df_DATA_spam],ignore_index=True)\n",
    "df_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Using the bag-of-words saved as df_DATA:\n",
    "\n",
    "    (a) make a Naive Bayes classification of all files inside the folders 'test_ham' and 'test_spam'\n",
    "\n",
    "    (b) summarize your findings in a Confusion matrix. What are your conclusions? Does it work well?\n",
    "\n",
    "    (c) repeat with different number of N_tokens_per_class. Change to N=10 and N=150, tokens instead of 40. What do you observe in the new Confusion matrices?\n",
    "    \n",
    "    (d) can you make any other modifications in the arguments of CountVectorizer, that can change the chosen TOKENS and improve the result?\n",
    "    \n",
    "    P.S. make use of any command you want from scikit-learn libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
