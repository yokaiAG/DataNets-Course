{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 08.Classification pt.B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name:\n",
    "\n",
    "Date: 15 November 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Naive Bayes (discrete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement Multi-variate Bernoulli NB for smap filtering.\n",
    "\n",
    "- Category $0$: Ham\n",
    "- Category $1$: Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.a - preprocess\n",
    "\n",
    "Use the collection of documents stored in train_ham, train_spam directories to select an appropriate set of features-words, based on frequency of appearence in the text. Select a maximum of $N=40$ tokens per category. You can use CountVectorizer from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_tokens_per_class = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Fishbone\n"
     ]
    }
   ],
   "source": [
    "# Your current path\n",
    "%cd\n",
    "#%cd data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Fishbone\n",
      "mkdir: data: File exists\n",
      "/Users/Fishbone/data\n",
      "\u001b[34m09_exercise_Spam\u001b[m\u001b[m/       09_exercise_Spam.zip.1  09_exercise_Spam.zip.3\n",
      "09_exercise_Spam.zip    09_exercise_Spam.zip.2  \u001b[34mdata\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "# change directory to where you will store the dataset\n",
    "%cd \n",
    "#YOUR PATH HERE\n",
    "%mkdir data\n",
    "%cd data\n",
    "%ls\n",
    "#!wget https://nuage.lip6.fr/s/bDM42aFWe5BxYRw/download/09_exercise_Spam.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzip\n",
    "#!tar -zxvf 09_exercise_Spam.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m09_exercise_Spam\u001b[m\u001b[m/       09_exercise_Spam.zip.1  09_exercise_Spam.zip.3\r\n",
      "09_exercise_Spam.zip    09_exercise_Spam.zip.2  \u001b[34mdata\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Fishbone/data/09_exercise_Spam\n"
     ]
    }
   ],
   "source": [
    "%cd 09_exercise_Spam/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Module\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text File\n",
    "\n",
    "def read_text_file(file_path,out):\n",
    "    with open(file_path,mode='r',encoding=\"utf8\", errors='ignore') as f:\n",
    "        data = f.read()\n",
    "        out.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First the train_ham**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_ham = '/train_ham'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the directory\n",
    "path = os.getcwd()+path_train_ham\n",
    "os.chdir(path)\n",
    "  \n",
    "# iterate through all TRAIN files\n",
    "TEXT_train_ham = []\n",
    "for file in os.listdir():\n",
    "    # Check whether file is in text format or not\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = f\"{path}/{file}\"\n",
    "  \n",
    "        # call read text file function\n",
    "        read_text_file(file_path,TEXT_train_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: summer cookout\n",
      "sally ,\n",
      "i had talked to ted and he mentioned the cookout you were planning for the ou\n",
      "group this summer . please let me help with any of the logistics ,\n",
      "set - up , and clean up . i know these things require a great deal of effort ,\n",
      "especially when you welcome people to your personal residence . i want to ,\n",
      "and insist on helping in whatever way i can . i am really grateful for what\n",
      "you have done for the university of oklahoma and i am also aware of\n",
      "the great sacrifices you make for enron . if possible , we might schedule a\n",
      "lunch to discuss the specifics . i would also be grateful if we could discuss\n",
      "my most recent opportunity at enron , texas risk . i realize the there is a\n",
      "great deal i can gain from this move and i would appreciate any insight on how\n",
      "to obtain the most from this experience .\n",
      "thanks\n",
      "jody\n"
     ]
    }
   ],
   "source": [
    "print(TEXT_train_ham[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=CountVectorizer(token_pattern=r'[A-Za-z]{4,10}',max_features=N_tokens_per_class,stop_words='english',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['attached', 'beck', 'business', 'corp', 'date', 'enron', 'following', 'forward', 'forwarded', 'global', 'group', 'help', 'houston', 'informatio', 'just', 'know', 'like', 'london', 'management', 'meeting', 'mike', 'need', 'office', 'process', 'questions', 'regards', 'report', 'review', 'risk', 'sally', 'sent', 'subject', 'team', 'thank', 'thanks', 'time', 'today', 'trading', 'week', 'work']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Fishbone/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['neverthele'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "X_ham=vectorizer.fit(TEXT_train_ham)\n",
    "TOKENS_ham = vectorizer.get_feature_names()\n",
    "print(TOKENS_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Then the train_spam**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Fishbone/data/09_exercise_Spam\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "path_train_spam = '/train_spam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the directory\n",
    "path = os.getcwd()+path_train_spam\n",
    "os.chdir(path)\n",
    "  \n",
    "# iterate through all files\n",
    "TEXT_train_spam = []\n",
    "for file in os.listdir():\n",
    "    # Check whether file is in text format or not\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = f\"{path}/{file}\"\n",
    "  \n",
    "        # call read text file function\n",
    "        read_text_file(file_path,TEXT_train_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: in the heart of your business !\n",
      "corporate image can say a lot of things about your\n",
      "company . contemporary rhythm of life is too dynamic . sometimes it takes only\n",
      "several seconds for your company to be remembered or to be lost amonq\n",
      "competitors . get your ioqo , business stationery or website done riqht\n",
      "now ! fast turnaround : you wiil see severai iogo variants in three\n",
      "business days . satisfaction guaranteed : we provide uniimited amount of\n",
      "changes ; you can be sure : it wili meet your needsand fit your\n",
      "business . fiexible discounts : iogo improvement , additional formats , buik\n",
      "orders , special packages . creative design for competitive price : have a look at it right\n",
      "now ! _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ not interested . . . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n"
     ]
    }
   ],
   "source": [
    "print(TEXT_train_spam[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['address', 'available', 'best', 'business', 'click', 'company', 'email', 'free', 'future', 'good', 'help', 'home', 'http', 'informatio', 'interested', 'just', 'know', 'life', 'like', 'list', 'mail', 'make', 'message', 'money', 'need', 'offer', 'online', 'order', 'people', 'profession', 'receive', 'save', 'send', 'site', 'software', 'subject', 'time', 'today', 'want', 'website']\n"
     ]
    }
   ],
   "source": [
    "X_spam=vectorizer.fit(TEXT_train_spam)\n",
    "TOKENS_spam = vectorizer.get_feature_names()\n",
    "print(TOKENS_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.b - done\n",
    "\n",
    "Then use the intersection of tokens from both categories to build a bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS = set(TOKENS_ham).union(set(TOKENS_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attached', 'business', 'need', 'subject', 'enron', 'trading', 'interested', 'company', 'want', 'corp', 'address', 'receive', 'send', 'date', 'questions', 'london', 'future', 'houston', 'just', 'group', 'time', 'email', 'mike', 'regards', 'online', 'sent', 'beck', 'know', 'today', 'meeting', 'week', 'forwarded', 'available', 'process', 'software', 'money', 'profession', 'order', 'help', 'sally', 'management', 'team', 'following', 'message', 'global', 'like', 'forward', 'site', 'life', 'informatio', 'review', 'website', 'home', 'office', 'mail', 'best', 'people', 'offer', 'http', 'free', 'risk', 'report', 'save', 'thanks', 'thank', 'click', 'list', 'good', 'make', 'work'} \n",
      "\n",
      "count_TOKENS= 70\n"
     ]
    }
   ],
   "source": [
    "print(TOKENS,'\\n')\n",
    "print('count_TOKENS=',len(TOKENS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common_TOKENS= {'business', 'need', 'subject', 'help', 'know', 'today', 'like', 'informatio', 'time', 'just'}\n"
     ]
    }
   ],
   "source": [
    "print('common_TOKENS=',set(TOKENS_ham).intersection(set(TOKENS_spam)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the set of TOKENS, to create a binary matrix for the Train and a binary matrix for the Test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "My_vectorizer = CountVectorizer(vocabulary=TOKENS,binary='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ham_my = My_vectorizer.fit_transform(TEXT_train_ham).toarray()\n",
    "X_spam_my = My_vectorizer.fit_transform(TEXT_train_spam).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 70)\n",
      "(3565, 70)\n"
     ]
    }
   ],
   "source": [
    "#Test if the size is correct\n",
    "print(np.shape(X_ham_my))\n",
    "print(np.shape(X_spam_my))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spam [Y=1]</th>\n",
       "      <th>attached</th>\n",
       "      <th>business</th>\n",
       "      <th>need</th>\n",
       "      <th>subject</th>\n",
       "      <th>enron</th>\n",
       "      <th>trading</th>\n",
       "      <th>interested</th>\n",
       "      <th>company</th>\n",
       "      <th>want</th>\n",
       "      <th>...</th>\n",
       "      <th>risk</th>\n",
       "      <th>report</th>\n",
       "      <th>save</th>\n",
       "      <th>thanks</th>\n",
       "      <th>thank</th>\n",
       "      <th>click</th>\n",
       "      <th>list</th>\n",
       "      <th>good</th>\n",
       "      <th>make</th>\n",
       "      <th>work</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4960</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4961</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4962</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4963</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4964</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4965 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Spam [Y=1]  attached  business  need  subject  enron  trading  \\\n",
       "0              0         0         0     0        0      0        0   \n",
       "1              0         0         0     0        0      0        0   \n",
       "2              0         0         0     0        0      0        0   \n",
       "3              0         0         0     0        1      0        1   \n",
       "4              0         0         0     0        1      0        0   \n",
       "...          ...       ...       ...   ...      ...    ...      ...   \n",
       "4960           1         1         0     0        0      0        0   \n",
       "4961           1         0         0     0        0      0        0   \n",
       "4962           1         1         1     0        0      0        0   \n",
       "4963           1         0         0     0        0      1        0   \n",
       "4964           1         0         0     0        0      1        0   \n",
       "\n",
       "      interested  company  want  ...  risk  report  save  thanks  thank  \\\n",
       "0              0        0     0  ...     0       0     0       1      1   \n",
       "1              0        0     0  ...     0       0     1       0      0   \n",
       "2              0        1     0  ...     1       0     1       1      1   \n",
       "3              0        0     1  ...     1       0     1       0      0   \n",
       "4              0        0     1  ...     1       0     1       0      0   \n",
       "...          ...      ...   ...  ...   ...     ...   ...     ...    ...   \n",
       "4960           1        0     0  ...     1       1     0       1      0   \n",
       "4961           0        0     0  ...     0       0     0       0      0   \n",
       "4962           0        0     0  ...     0       1     0       0      1   \n",
       "4963           1        0     0  ...     0       0     0       0      0   \n",
       "4964           0        0     0  ...     0       0     0       0      1   \n",
       "\n",
       "      click  list  good  make  work  \n",
       "0         0     0     0     0     0  \n",
       "1         0     1     0     0     0  \n",
       "2         1     0     0     1     1  \n",
       "3         0     0     0     0     1  \n",
       "4         0     0     0     0     1  \n",
       "...     ...   ...   ...   ...   ...  \n",
       "4960      0     0     1     0     0  \n",
       "4961      0     0     0     0     0  \n",
       "4962      0     0     0     1     0  \n",
       "4963      0     0     0     0     0  \n",
       "4964      0     0     0     0     0  \n",
       "\n",
       "[4965 rows x 71 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_DATA_ham = pd.DataFrame(X_ham_my,columns=TOKENS)\n",
    "df_DATA_ham.insert(0, 'Spam [Y=1]',0)\n",
    "df_DATA_spam = pd.DataFrame(X_spam_my,columns=TOKENS) \n",
    "df_DATA_spam.insert(0, 'Spam [Y=1]',1)\n",
    "df_DATA = pd.concat([df_DATA_ham,df_DATA_spam],ignore_index=True)\n",
    "df_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Using the bag-of-words saved as df_DATA:\n",
    "\n",
    "    (a) make a Naive Bayes classification of all files inside the folders 'test_ham' and 'test_spam'\n",
    "\n",
    "    (b) summarize your findings in a Confusion matrix. What are your conclusions? Does it work well?\n",
    "\n",
    "    (c) repeat with different number of N_tokens_per_class. Change to N=10 and N=150, tokens instead of 40. What do you observe in the new Confusion matrices?\n",
    "    \n",
    "    (d) can you make any other modifications in the arguments of CountVectorizer, that can change the chosen TOKENS and improve the result?\n",
    "    \n",
    "    (e) try working with the command sklearn.feature_extraction.text.TfidfVectorizer instead of the CountVectorizer. How would the tf-idf matrix modify the results?\n",
    "    \n",
    "    P.S. make use of any command you want from scikit-learn libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
