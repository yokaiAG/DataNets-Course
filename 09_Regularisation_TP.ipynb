{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Feature Selection / Regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### author: Anastasios Giovanidis, 2019-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the TP related to Feature Selection / Regularisation. We will need to import the following libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston, load_diabetes, load_linnerud\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.cross_decomposition import PLSRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this class we will learn how to use PYTHON for regression, using the new methods that either choose a subset of features, or reduce the dimension, or shrink the coefficients. Specifically, we will study:\n",
    "\n",
    "- Ridge Regression\n",
    "- The Lasso\n",
    "- Partial Least Squares (PLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation: Importing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, we will first import a dataset, with a large number of features. We need the following libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset 1: Boston House Prices dataset (Linear Regression example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_boston(): from the sklearn.datasets library\n",
    "boston=load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can read the general discription of the \"Boston\" dataset and find the names of Features by printing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])\n"
     ]
    }
   ],
   "source": [
    "print(boston.keys())\n",
    "#print(boston.DESCR)\n",
    "#print(boston.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a panda DataFrame to keep all Features in a compact format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df=pd.DataFrame(boston.data,columns=boston.feature_names)\n",
    "#print(boston_df)\n",
    "#print(boston_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response Y is actually the price of an asset found in the dataset as 'target'. We need to add this as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df['Price']=boston.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (Ridge Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the Boston dataset to perform Linear Regression, and afterwards, Ridge Regression with the sklearn functions. For Ridge use $\\lambda = 5$ (the sklearn Ridge parameter is called alpha). Compare the two methods, based on the Mean Squared Error, $R^2$ and the coefficient values. What do you observe?\n",
    "\n",
    "\n",
    "- As a second step, plot (i) the MSE test error and (ii) the values of feature coefficients, for $\\lambda\\in[0,25]$.\n",
    "\n",
    "note 1: To use Ridge, you need to **rescale** the data (not the target) so that the mean is $0$ and the standard deviation is $1$.\n",
    "\n",
    "note 2: You should also consider splitting the dataset into a Train and a Test set.\n",
    "\n",
    "note 3: The Ridge Regression parameter is 'alpha'. For documentation, see: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparation: Rescale + Train-Test Split. We will first need to rescale and standardize the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale: from the preprocessing library\n",
    "Xboston = scale(boston_df.drop('Price',axis=1))\n",
    "Yboston = boston_df['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.  0.  0. -0. -0. -0. -0. -0. -0.  0. -0. -0. -0.]\n"
     ]
    }
   ],
   "source": [
    "print(Xboston.mean(0).round())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split the dataset in Train-Test (Validation set, with 30% test dataset, randomly chosen - with seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split: from the sklearn.model_selection library\n",
    "Xbo_train, Xbo_test, ybo_train, ybo_test = train_test_split(Xboston, Yboston, test_size=0.3, random_state=10)\n",
    "#print(len(ybo_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LinearRegression(): from the sklearn.linear_model library.\n",
    "lr_boston = LinearRegression()\n",
    "lr_boston.fit(Xbo_train, ybo_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ybo_pred_lr = lr_boston.predict(Xbo_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_squared_error (LR): 29.33 \n",
      "\n",
      "R2_score (LR): 0.70 \n",
      "\n",
      "Coefficients (LR): [-1.32140428  1.51483181 -0.16626579  0.41157944 -1.77116763  2.35282088\n",
      "  0.31849871 -3.25664538  2.63257618 -2.05946639 -1.75520086  1.18114343\n",
      " -3.88704342]\n"
     ]
    }
   ],
   "source": [
    "# mean_squared_error, r2_score: from the sklearn.metrics library.\n",
    "# The mean squared error\n",
    "print(\"Mean_squared_error (LR): %.2f\" % mean_squared_error(ybo_test, ybo_pred_lr),'\\n')\n",
    "# Explained variance score: 1 is perfect prediction \n",
    "print('R2_score (LR): %.2f' % r2_score(ybo_test, ybo_pred_lr),'\\n')\n",
    "# Coefficients\n",
    "print('Coefficients (LR):', lr_boston.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge: from the sklearn.linear_model library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (The Lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the same Boston dataset, with same Train-test split, to perform the Lasso using the sklearn function. Use $\\lambda = 1$ (the sklearn Lasso parameter is called alpha). Print the Mean Squared Error, $R^2$ and the coefficient values. What do you observe?\n",
    "\n",
    "\n",
    "- As a second step, plot (i) the MSE test error and (ii) the values of feature coefficients, for $\\lambda\\in[0,0.3]$.\n",
    "\n",
    "\n",
    "- Finally, compare the minimum Test MSE for Lasso, with the one for Ridge and the OLS. Which one is better?\n",
    "\n",
    "note 1: To use Lasso, you need keep the **rescaled** data from before. \n",
    "\n",
    "note 2: Keep the same split of the dataset as above in a Train and a Test set.\n",
    "\n",
    "note 3: The Lasso Regression parameter is 'alpha'. For documentation, see: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso: from the sklearn.linear_model library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the comparison between OLS, Ridge and Lasso, using the two other datasets, i.e. 'load_diabetes', 'load_linnerud'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 (Partial Least Squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same dataset to perform Partial Least Squares (PLS). \n",
    "\n",
    "- Find the Test MSE for all possible number of linear-combinations of features. Use the Train-Test split as in the previous exercises. Which number $M\\in 1,\\ldots,p$ of elements gives the minimum Test MSE? Compare with OLS, Ridge, Lasso.\n",
    "\n",
    "- Instead of a single Train-Test split, use the Cross-Validation framework, making use of the entire available trace. What number of linear combinations of the original features, $M\\in 1,\\ldots,p$ gives the optimal Test MSE result?\n",
    "\n",
    "note 1: You will need the 'cross_validate' feature, from sklearn.model_selection : https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html for documentation.\n",
    "\n",
    "note 2: You will need the 'PLSRegression' feature from sklearn.cross_decomposition : https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html for documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first use the same Train-Test split as before, with Test equal to 30% of the original dataset.\n",
    "# apply: PLSRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use Cross-Validation on the entire dataset.\n",
    "# apply: PLSRegression, cross_validate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
